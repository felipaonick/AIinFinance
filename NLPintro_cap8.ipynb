{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS3uBJa5-7gI",
        "outputId": "93c00504-0118-48b8-aae0-f96c9bce10e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'a', 'tokenize', 'test']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "#nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "#tokenizzazione\n",
        "\n",
        "#text to tokenize\n",
        "text = \"This is a tokenize test\"\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizzazione con TextBlob\n",
        "TextBlob(text).words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PvFF1nUAAU1",
        "outputId": "2d669fa5-7545-4097-82f3-1a7858d7b379"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['This', 'is', 'a', 'tokenize', 'test'])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"S&P and NASDAQ are the two most popular indices in US\"\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "text_tokens = word_tokenize(text)\n",
        "tokens_without_sw= [word for word in text_tokens if not word in stop_words]\n",
        "\n",
        "print(tokens_without_sw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlEsBTfoArL4",
        "outputId": "177ae699-f111-45d8-b7ed-b5272b9d50e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['S', '&', 'P', 'NASDAQ', 'two', 'popular', 'indices', 'US']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"It's a Stemming testing\"\n",
        "\n",
        "parsed_text = word_tokenize(text)\n",
        "\n",
        "# Initialize stemmer.\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "# Stem each word.\n",
        "[(word, stemmer.stem(word)) for i, word in enumerate(parsed_text)\n",
        " if word.lower() != stemmer.stem(parsed_text[i])]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxR4UR4mBYZs",
        "outputId": "f2d8419f-2e0f-41ec-a59a-821217ed5a83"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Stemming', 'stem'), ('testing', 'test')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This world has a lot of faces \"\n",
        "\n",
        "from textblob import Word\n",
        "nltk.download('wordnet')\n",
        "parsed_data= TextBlob(text).words\n",
        "[(word, word.lemmatize()) for i, word in enumerate(parsed_data)\n",
        " if word != parsed_data[i].lemmatize()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asslN09JBike",
        "outputId": "78a1325b-e738-4ce2-bee0-af7c9a9a3c20"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('has', 'ha'), ('faces', 'face')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Google is looking at buying U.K. startup for $1 billion'\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "TextBlob(text).tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kTxkdz9Bz7y",
        "outputId": "681b61d6-bab1-43bd-e7dd-25c10f7e7ce3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Google', 'NNP'),\n",
              " ('is', 'VBZ'),\n",
              " ('looking', 'VBG'),\n",
              " ('at', 'IN'),\n",
              " ('buying', 'VBG'),\n",
              " ('U.K.', 'NNP'),\n",
              " ('startup', 'NN'),\n",
              " ('for', 'IN'),\n",
              " ('1', 'CD'),\n",
              " ('billion', 'CD')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Google is looking at buying U.K. startup for $1 billion'\n",
        "\n",
        "for entity in nlp(text).ents:\n",
        "    print(\"Entity: \", entity.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpllCgcSCAV3",
        "outputId": "8740c768-3a0b-4eeb-b406-c3da74759109"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity:  Google\n",
            "Entity:  U.K.\n",
            "Entity:  $1 billion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy \n",
        "displacy.render(nlp(text), style=\"ent\", jupyter = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "fElLCyuQCRIo",
        "outputId": "9a9aa48f-08d6-48a3-847d-317a9f6419cf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Google\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is looking at buying \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    U.K.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " startup for \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $1 billion\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Google is looking at buying U.K. startup for $1 billion'\n",
        "doc = nlp(text)\n",
        "import pandas as pd\n",
        "pd.DataFrame([[t.text, t.is_stop, t.lemma_, t.pos_]\n",
        "              for t in doc],\n",
        "             columns=['Token', 'is_stop_word', 'lemma', 'POS'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "DHyvJD09CRqn",
        "outputId": "00e50196-dd62-4684-8c5d-abb17edb7883"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Token  is_stop_word    lemma    POS\n",
              "0    Google         False   Google  PROPN\n",
              "1        is          True       be    AUX\n",
              "2   looking         False     look   VERB\n",
              "3        at          True       at    ADP\n",
              "4    buying         False      buy   VERB\n",
              "5      U.K.         False     U.K.  PROPN\n",
              "6   startup         False  startup   NOUN\n",
              "7       for          True      for    ADP\n",
              "8         $         False        $    SYM\n",
              "9         1         False        1    NUM\n",
              "10  billion         False  billion    NUM"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-49c4052c-38db-4a15-9764-0812bacbc59f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Token</th>\n",
              "      <th>is_stop_word</th>\n",
              "      <th>lemma</th>\n",
              "      <th>POS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Google</td>\n",
              "      <td>False</td>\n",
              "      <td>Google</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is</td>\n",
              "      <td>True</td>\n",
              "      <td>be</td>\n",
              "      <td>AUX</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>looking</td>\n",
              "      <td>False</td>\n",
              "      <td>look</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>at</td>\n",
              "      <td>True</td>\n",
              "      <td>at</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>buying</td>\n",
              "      <td>False</td>\n",
              "      <td>buy</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>U.K.</td>\n",
              "      <td>False</td>\n",
              "      <td>U.K.</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>startup</td>\n",
              "      <td>False</td>\n",
              "      <td>startup</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>for</td>\n",
              "      <td>True</td>\n",
              "      <td>for</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>$</td>\n",
              "      <td>False</td>\n",
              "      <td>$</td>\n",
              "      <td>SYM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>NUM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>billion</td>\n",
              "      <td>False</td>\n",
              "      <td>billion</td>\n",
              "      <td>NUM</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49c4052c-38db-4a15-9764-0812bacbc59f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-49c4052c-38db-4a15-9764-0812bacbc59f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-49c4052c-38db-4a15-9764-0812bacbc59f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "'The stock price of google jumps on the earning data today',\n",
        "'Google plunge on China Data!'\n",
        "]\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "print( vectorizer.fit_transform(sentences).todense() )\n",
        "print( vectorizer.vocabulary_ )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8Xv3pVzCl2l",
        "outputId": "bb0dc1a2-93db-4c62-bc2c-7ba39e85d8cc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 1 1 1 1 1 0 1 1 2 1]\n",
            " [1 1 0 1 0 0 1 1 0 0 0 0]]\n",
            "{'the': 10, 'stock': 9, 'price': 8, 'of': 5, 'google': 3, 'jumps': 4, 'on': 6, 'earning': 2, 'data': 1, 'today': 11, 'plunge': 7, 'china': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "TFIDF = vectorizer.fit_transform(sentences)\n",
        "print(vectorizer.get_feature_names_out()[-10:])\n",
        "print(TFIDF.shape)\n",
        "print(TFIDF.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7OxiA3ZCy52",
        "outputId": "7fde4562-b9d5-4a38-a3af-aa789d1d6982"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['china' 'data' 'earning' 'google' 'jumps' 'plunge' 'price' 'stock'\n",
            " 'today']\n",
            "(2, 9)\n",
            "[[0.         0.29017021 0.4078241  0.29017021 0.4078241  0.\n",
            "  0.4078241  0.4078241  0.4078241 ]\n",
            " [0.57615236 0.40993715 0.         0.40993715 0.         0.57615236\n",
            "  0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Apple orange cats dogs\")\n",
        "print(\"Vector representation of the sentence for first 10 features: \\n\",\\\n",
        "doc.vector[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BP1jygHmDdJW",
        "outputId": "01be32ed-f1a4-4d04-d7c2-21060593205e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector representation of the sentence for first 10 features: \n",
            " [-0.43863457 -0.3626267  -0.20616335  1.0866606   0.44007337 -0.03703639\n",
            "  0.7782439   0.55959123  0.21888082 -0.40239647]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "sentences = [\n",
        "['The','stock','price', 'of', 'Google', 'increases'],\n",
        "['Google','plunge',' on','China',' Data!']]\n",
        "\n",
        "# train model\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "\n",
        "# summarize the loaded model\n",
        "words = list(model.wv.key_to_index)\n",
        "print(words)\n",
        "print(model.wv.get_vector('Google')[1:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiiVy0YKDiG8",
        "outputId": "b888f1f3-ab55-48b5-f29b-0987547f123f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Google', ' Data!', 'China', ' on', 'plunge', 'increases', 'of', 'price', 'stock', 'The']\n",
            "[ 0.00023643  0.00510335  0.00900927 -0.00930295]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "'The stock price of google jumps on the earning data today',\n",
        "'Google plunge on China Data!']\n",
        "sentiment = (1, 0)\n",
        "data = pd.DataFrame({'Sentence':sentences,\n",
        "        'sentiment':sentiment})\n",
        "\n",
        "# feature extraction\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vect = CountVectorizer().fit(data['Sentence'])\n",
        "X_train_vectorized = vect.transform(data['Sentence'])\n",
        "\n",
        "# Running naive bayes model\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clfrNB = MultinomialNB(alpha=0.1)\n",
        "clfrNB.fit(X_train_vectorized, data['sentiment'])\n",
        "\n",
        "#Testing the model\n",
        "preds = clfrNB.predict(vect.transform(['Apple price plunge',\\\n",
        " 'Amazon price jumps']))\n",
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQg879udHC_p",
        "outputId": "08b7faae-740b-46da-d81b-8bf57c44f373"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "'The stock price of google jumps on the earning data today',\n",
        "'Google plunge on China Data!'\n",
        "]\n",
        "\n",
        "#Getting the bag of words\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "vect=CountVectorizer(ngram_range=(1, 1),stop_words='english')\n",
        "sentences_vec=vect.fit_transform(sentences)\n",
        "\n",
        "#Running LDA on the bag of words.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "lda=LatentDirichletAllocation(n_components=3)\n",
        "lda.fit_transform(sentences_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzpvhpygHYF9",
        "outputId": "8eaf5a12-f58b-45a7-9080-14c2060a7db5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.04311114, 0.91377772, 0.04311114],\n",
              "       [0.06869319, 0.86261362, 0.06869319]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}